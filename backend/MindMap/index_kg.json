{"index_struct_id": "e263cade-3291-4159-b045-a2c4af0837ab", "docstore": {"docs": {"e263cade-3291-4159-b045-a2c4af0837ab": {"text": null, "doc_id": "e263cade-3291-4159-b045-a2c4af0837ab", "embedding": null, "extra_info": null, "table": {"you": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "classifier": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c", "9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "height": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "classification": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "training set": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "5'5\" women": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "5'5\" men": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "feature space": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "two training points": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "probabilistic classifier": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "desirable": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "false negative": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "somebody misses an early diagnosis": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "false positive": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "somebody gets an unnecessary treatment": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "loss function": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "badness": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "Bayes risk": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "risk of the Bayes classifier": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "TEX": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "computer software": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "Kaggle": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c", "9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "online platform": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "submitting to Kaggle": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "you to get feedback": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "consequences of academic misconduct": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "particularly severe": ["021795a4-a1aa-43cb-90f9-b84e15d49a6c"], "file": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "decision boundary": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "x=b": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "value": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "part 1": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "isocontours": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "functions": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "domain": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "characteristics": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "eigenvectors": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "eigenvalues": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "covariance matrix": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "singular": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "Gaussian discriminant analysis": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "mean": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "PDF": ["9b7a0ff5-9bd5-4808-a748-6a742a2f2429"], "Kaggle competition": ["176ff7c6-17f6-47ef-9267-35ba18621133"], "gradient descent": ["176ff7c6-17f6-47ef-9267-35ba18621133"], "linear decision function": ["176ff7c6-17f6-47ef-9267-35ba18621133"], "Bag-Of-Words model": ["176ff7c6-17f6-47ef-9267-35ba18621133"], "optimization methods": ["176ff7c6-17f6-47ef-9267-35ba18621133"], "stochastic gradient descent": ["176ff7c6-17f6-47ef-9267-35ba18621133"], "line search algorithm": ["176ff7c6-17f6-47ef-9267-35ba18621133"], "quadratic program": ["176ff7c6-17f6-47ef-9267-35ba18621133"], "support vector machine": ["176ff7c6-17f6-47ef-9267-35ba18621133"], "decision theory": ["5d8fccea-cbf6-4c4f-9ca6-d87897c9e5ef"], "Bayes": ["5d8fccea-cbf6-4c4f-9ca6-d87897c9e5ef"], "Lecture 5": ["b5619965-5220-4f6d-8065-59880da12e08"], "Machine learning abstractions": ["b5619965-5220-4f6d-8065-59880da12e08"], "unconstrained": ["b5619965-5220-4f6d-8065-59880da12e08"], "constrained": ["b5619965-5220-4f6d-8065-59880da12e08"], "linear programs": ["b5619965-5220-4f6d-8065-59880da12e08"], "quadratic programs": ["b5619965-5220-4f6d-8065-59880da12e08"], "convex programs": ["b5619965-5220-4f6d-8065-59880da12e08"], "Lecture 6": ["b5619965-5220-4f6d-8065-59880da12e08"], "the Bayes decision rule": ["b5619965-5220-4f6d-8065-59880da12e08"], "optimal risk": ["b5619965-5220-4f6d-8065-59880da12e08"], "CS 189/289A Introduction to Machine Learning": ["9a161fa2-669f-4b90-8b28-76c8cac0586b"], "class": ["9a161fa2-669f-4b90-8b28-76c8cac0586b"], "Jonathan Shewchuk": ["9a161fa2-669f-4b90-8b28-76c8cac0586b"], "Mondays and Wednesdays": ["9a161fa2-669f-4b90-8b28-76c8cac0586b"], "January 18": ["9a161fa2-669f-4b90-8b28-76c8cac0586b"], "January 24": ["9a161fa2-669f-4b90-8b28-76c8cac0586b"], "that can be viewed by all the TAs": ["9a161fa2-669f-4b90-8b28-76c8cac0586b"], "TBA and by appointment": ["9a161fa2-669f-4b90-8b28-76c8cac0586b"]}, "text_chunks": {"021795a4-a1aa-43cb-90f9-b84e15d49a6c": {"text": "I want to give you four levels of abstraction that can help you think about machine learning.\n\nAs a data scientist, you might be given an application, and your challenge is to turn it into an optimization problem that we know how to solve.\n\nSometimes you get a big improvement by tailoring the model or its features to \ufb01t the structure of your speci\ufb01c data.\n\nThe model also has a big e \u21b5 ect on whether you over\ufb01t or under\ufb01t.\n\nAnd if you want a model that you can interpret so you can do inference , the model has to have a simple structure.\n\nLastly, you have to pick a model that leads to an optimization problem that can be solved.\n\nNevertheless, for everything you learn in this class, think about where it \ufb01ts in this hierarchy.\n\nAn important skill for you to develop is to be able to go from an application to a well-de\ufb01ned optimization problem.\n\nFind w that minimizes (or maximizes) a continuous objective fn f ( w ).\n\nA global minimum of f is a value w such that f ( w ) \uf8ff f ( v ) for every v .\n\nE.g., perceptron risk fn is convex and nonsmooth.\n\nThese algs \ufb01nd a local minimum.\n\nUse Lagrange multipliers.\n\nLinear objective fn + linear inequality constraints.\n\nThe set of points w that satisfy all constraints is a convex polytope called the feasible region F [shaded].\n\nThe optimum is the point in F that is furthest in the direction c .\n\nA point set P is convex if for every p , q 2 P , the line segment with endpoints p , q lies entirely in P .\n\nThe optimum achieves equality for some constraints (but not most), called the active constraints of the optimum.\n\nIn the \ufb01gure above, there are two active constraints.\n\nFor example, in the \ufb01gure above, if c pointed straight up, every point on the top horizontal edge would be optimal.\n\nThe data are linearly separable i \u21b5 the feasible region is not the empty set. !\n\nIf you knew which constraints would be the active constraints once you found the solution, it would be easy; the hard part is \ufb01guring out which constraints should be the active ones.\n\nThere are exponentially many possibilities, so you can\u2019t a \u21b5 ord to try them all.\n\nQuadratic, convex objective fn + linear inequality constraints.\n\nQw > 0 for all w , 0.\n\nOnly one local minimum!\n\nIf Q is inde\ufb01nite, then f is not convex, the minimum is not always unique, and quadratic programming is NP-hard.\n\nFind maximum margin classi\ufb01er.\n\nNumerical optimization @ Berkeley: EECS 127 / 227AT / 227BT / 227C.\n\nSuppose your data is adult men and women with just one feature: their height.\n\nYou want to train a classi\ufb01er that takes in an adult\u2019s height and returns a classi\ufb01cation, man or woman.\n\nSuppose you are asked to predict the sex of a 5\u20195\u201d adult.\n\nWell, your training set includes some 5\u20195\u201d women and some 5\u20195\u201d men.\n\nWhat should you do?] [In your feature space, you have two training points at the same location with di \u21b5 erent classes.\n\nMore generally, the height distributions of men and women overlap.\n\nMultiple sample points with di \u21b5 erent classes could lie at same point: we want a probabilistic classi\ufb01er.\n\nSuppose 10% of population has cancer, 90% doesn\u2019t.\n\nP (1,200 \uf8ff X \uf8ff 1,600) = 0 .\n\nYou meet guy eating x = 1,400 calories / day.\n\nGuess whether he has cancer?\n\nP ( X ) = 0 .\n\nP ( X ) = 0 .\n\nP (cancer 1,200 \uf8ff X \uf8ff 1,600 cals) = 5 / 14 \u21e1 36%.\n\nBut that\u2019s not always the right assumption.\n\nIf you\u2019re developing a cheap screening test for cancer, you\u2019d rather have more false positives and fewer false negatives.\n\nA false negative might mean somebody misses an early diagnosis and dies of a cancer that could have been treated if caught early.\n\nA loss function L ( z , y ) speci\ufb01es badness if classi\ufb01er predicts z , true class is y .\n\nA 36% probability of loss 5 is worse than a 64% prob.\n\nThe Bayes risk, aka optimal risk, is the risk of the Bayes classi\ufb01er.\n\nSuppose X has a continuous probability density fn (PDF).\n\nIn other words, suppose you want a classi\ufb01er that maximizes the chance of a correct prediction.\n\nThe wrong answer would be to look where these two curves cross and make that be the decision boundary.\n\nDe\ufb01ne risk as before, replacing summations with integrals.\n\nThe Bayesian approach is a particularly convenient way to generate multi-class classi\ufb01ers, because you can simply choose whichever class has the greatest posterior probability.\n\nThere is usually error in all of these probabilities.\n\nTEX or other word processing software.\n\nTEX, one of the crown jewels of computer science, now is a good time!\n\nNeatly handwritten and scanned solutions will also be accepted.\n\nSome of the ma- terial may not have been covered in lecture; you are responsible for finding resources to understand it.\n\nSubmit your predictions for the test sets to Kaggle as early as possible.\n\nInclude your Kaggle scores in your write-up.\n\nWrite-up: Submit your solution in PDF format to \u201cHomework 3 Write-Up\u201d in Gradescope.\n\nIf there are graphs, include those graphs on the same pages as the question write-up.\n\nDO NOT put them in an appendix.\n\nWe need each solution to be self-contained on pages of its own.\n\nTEX or Word to typeset your solution.\n\nYou may also scan a neatly handwritten solution to produce the PDF.\n\nBegin code for each coding question in a fresh page.\n\nDo not put code from multiple questions in the same page.\n\nWhen you upload this PDF on Gradescope, make sure that you assign the relevant pages of your code from appendix to correct questions.\n\nCopying the answers or code of another student is strictly forbidden.\n\nFurthermore, all external material (i.e., anything outside lectures and assigned readings, including figures and pictures) should be cited properly.\n\nWe wish to remind you that consequences of academic misconduct are particularly severe !\n\nCode: Submit your code as a .zip file to \u201cHomework 3 Code\u201d.\n\nThis ensures your results are replicated when readers run your code.\n\nFor example, you can seed numpy with np.random.seed(42) .\n\nSupply instructions on how to add data to your code.\n\nCode requiring exorbitant memory or execution time might not be considered.\n\nCode submitted here must match that in the PDF Write-up.\n\nThe Kaggle score will not be accepted if the code provided a) does not compile or b) compiles but does not produce the file submitted to Kaggle.\n\nFind the Bayes optimal decision boundary and the corresponding Bayes decision rule by finding the point(s) at which the posterior probabilities are equal.\n\nUse the 0-1 loss function.\n\nSuppose the decision boundary for your classifier is x = b .\n\nHow does this value compare to that found in part 1?\n\nWrite code to plot the isocontours of the following functions, each on its own separate figure.\n\nMake sure it is clear which figure belongs to which part.\n\nYou\u2019re free to use any plotting libraries or stats utilities available in your programming language; for instance, in Python you can use Matplotlib and SciPy.\n\nChoose the boundaries of the domain you plot large enough to show the interesting characteristics of the isocontours", "doc_id": "021795a4-a1aa-43cb-90f9-b84e15d49a6c", "embedding": null, "extra_info": null, "index": 0, "child_indices": [], "ref_doc_id": "bfac5f31-d3df-41d7-87f7-aa808e930d69", "node_info": {"start": 0, "end": 6948}}, "9b7a0ff5-9bd5-4808-a748-6a742a2f2429": {"text": "the file submitted to Kaggle.\n\nFind the Bayes optimal decision boundary and the corresponding Bayes decision rule by finding the point(s) at which the posterior probabilities are equal.\n\nUse the 0-1 loss function.\n\nSuppose the decision boundary for your classifier is x = b .\n\nHow does this value compare to that found in part 1?\n\nWrite code to plot the isocontours of the following functions, each on its own separate figure.\n\nMake sure it is clear which figure belongs to which part.\n\nYou\u2019re free to use any plotting libraries or stats utilities available in your programming language; for instance, in Python you can use Matplotlib and SciPy.\n\nChoose the boundaries of the domain you plot large enough to show the interesting characteristics of the isocontours (use your judgment).\n\nMake sure we can tell what isovalue each contour is associated with\u2014you can do this with labels or a colorbar / legend.\n\nNo, it is not itself a valid PDF.\n\nIn your code, make sure to choose and set a fixed random number seed for whatever random number generator you use, so your simulation is reproducible, and document your choice of random number seed and random number generator in your write-up.\n\nFor each of the following parts, include the corresponding output of your program.\n\nCompute the 2 \u00d7 2 covariance matrix of the sample (based on the sample mean, not the true mean\u2014which you would not know given real-world data).\n\nCompute the eigenvectors and eigenvalues of this covariance matrix.\n\nThe eigenvector arrows should orig- inate at the mean and have magnitudes equal to their corresponding eigenvalues.\n\nNot doing that may lead to hours of frustration!\n\nPlot these rotated points on a new two dimensional-grid, again with both axes having range [ \u2212 15 , 15].\n\nIn your plots, clearly label the axes and include a title .\n\nMoreover, make sure the horizontal and vertical axis have the same scale!\n\nThe aspect ratio should be one.\n\nSuppose we have a classification problem with classes labeled 1 , . . .\n\nHence the risk of classifying a new data point x as class i \u2208 { 1 , 2 , . . .\n\nL ( r ( x ) = i , y = j ) P ( Y = j x ) .\n\nTo be clear, the actual label Y can never be c + 1.\n\nExplain why this is consistent with what one would expect intuitively.\n\nEvery sample point comes from a distribution with a di ff erent variance .) Note the word \u201cunivariate\u201d; we are working in dimension d = 1, and our \u201cpoints\u201d are real numbers.\n\nDerive the maximum likelihood estimates, denoted \u02c6 \u00b5 and \u02c6 \u03c3 , for the mean \u00b5 and the pa- rameter \u03c3 .\n\nShow all your work.\n\nGiven the true value of a statistic \u03b8 and an estimator \u02c6 \u03b8 of that statistic, we define the bias of the estimator to be the the expected di ff erence from the true value.\n\nWe say that an estimator is unbiased if its bias is 0.\n\nEither prove or disprove the following statement: The MLE sample estimator \u02c6 \u00b5 is unbiased .\n\nThe estimate \u02c6 \u03a3 makes sense as an approximation of \u03a3 only if \u02c6 \u03a3 is invertible.\n\nUnder what circumstances is \u02c6 \u03a3 not invertible?\n\nMake sure your answer is complete; i.e., it includes all cases in which the covariance matrix of the sample is singular.\n\nSuggest a way to fix a singular covariance matrix estimator \u02c6 \u03a3 by replacing it with a similar but invertible matrix.\n\nYour suggestion may be a kludge, but it should not change the covariance matrix too much.\n\nNote that infinitesimal numbers do not exist; if your solution uses a very small number, explain how to calculate a number that is su ffi ciently small for your purposes.\n\nConsider the normal distribution N (0 , \u03a3 ) with mean \u00b5 = 0.\n\nConsider all vectors of length 1; i.e., any vector x for which \u2225 x \u2225 = 1.\n\nWhich vector(s) x of length 1 maximizes the PDF f ( x )?\n\nWhich vector(s) x of length 1 minimizes f ( x )?\n\nYour answers should depend on the properties of \u03a3 .\n\nExplain your answer.\n\nFirst, compute the vari- ance of p .\n\nIn this problem, you will build classifiers based on Gaussian discriminant analysis.\n\nUnlike Home- work 1, you are NOT allowed to use any libraries for out-of-the-box classification (e.g. sklearn ).\n\nYou may use anything in numpy and scipy .\n\nThe training and test data can be found with this homework.\n\nDo NOT use the training / test data from Homework 1, as they have changed for this homework.\n\nThe starter code is similar to HW1\u2019s; we provide check .\n\nSubmit your predicted class labels for the test data on the Kaggle competition website and be sure to include your Kaggle display name and scores in your writeup.\n\nAlso be sure to include an appendix of your code at the end of your writeup.\n\nTaking pixel values as features (no new features yet, please), fit a Gaussian distribution to each digit class using maximum likelihood estimation.\n\nThis involves computing a mean and a covariance matrix for each digit class, as discussed in Lecture 9.\n\nYou may, and probably should, contrast-normalize the images before using their pixel values.\n\nWritten answer + graph) Visualize the covariance matrix for a particular class (digit).\n\nHow do the diagonal terms compare with the o ff -diagonal terms?\n\nWhat do you conclude from this?\n\nClassify the digits in the test set on the basis of posterior probabilities with two di ff erent approaches.\n\nIn your implementation, you might run into issues of determinants overflowing or under- flowing, or normal PDF probabilities underflowing.\n\nThese problems might be solved by learning about numpy.linalg.slogdet and / or scipy.stats.multivariate normal. logpdf .\n\nYou should not compute the inverse of \u03a3 (nor the determinant of \u03a3 ) as it is not guaranteed to be invertible.\n\nInstead, you should find a way to solve the implied linear system without computing the inverse.\n\nHold out 10,000 randomly chosen training points for a validation set.\n\nClassify each image in the validation set into one of the 10 classes.\n\nIf any of these covariance matrices turn out singular, implement the trick you described in Q7(b).\n\nYou are welcome to use validation to choose the right constant(s) for that trick.) Repeat the same tests and error rate calculations you did for LDA.\n\nPlot all the 10 curves on the same graph as shown in Figure 1.\n\nWhich digit is easiest to classify?\n\nWrite down your answer and suggest why you think it\u2019s the easiest digit.\n\nWritten answer) Using the mnist-data-hw3.npz , train your best classifier for the training data and classify the images in the test data .\n\nSubmit your labels to the online Kaggle compe- tition.\n\nRecord your optimum prediction rate in your submission and include your Kaggle username.\n\nYou are welcome to compute extra features for the Kaggle competition, as long as they do not use an exterior learned model for their computation (no transfer learning!).\n\nIf you do so, please describe your implementation in your assignment.\n\nPlease use extra features only for the Kaggle portion of the assignment.\n\nWritten answer) Next, apply LDA or QDA (your choice) to spam ( spam-data-hw3.npz ).\n\nSubmit your test results to the online Kaggle competition.\n\nRecord your optimum prediction rate in your submission.\n\nIf you use additional features (or omit features), please describe them.\n\nWe include a featurize.py file (similar to HW1\u2019s) that you may modify to create new features.\n\nIf you use the defaults, expect relatively low classification rates.\n\nWe suggest using a", "doc_id": "9b7a0ff5-9bd5-4808-a748-6a742a2f2429", "embedding": null, "extra_info": null, "index": 1, "child_indices": [], "ref_doc_id": "bfac5f31-d3df-41d7-87f7-aa808e930d69", "node_info": {"start": 6303, "end": 13608}}, "176ff7c6-17f6-47ef-9267-35ba18621133": {"text": "Kaggle competition, as long as they do not use an exterior learned model for their computation (no transfer learning!).\n\nIf you do so, please describe your implementation in your assignment.\n\nPlease use extra features only for the Kaggle portion of the assignment.\n\nWritten answer) Next, apply LDA or QDA (your choice) to spam ( spam-data-hw3.npz ).\n\nSubmit your test results to the online Kaggle competition.\n\nRecord your optimum prediction rate in your submission.\n\nIf you use additional features (or omit features), please describe them.\n\nWe include a featurize.py file (similar to HW1\u2019s) that you may modify to create new features.\n\nIf you use the defaults, expect relatively low classification rates.\n\nWe suggest using a Bag-Of-Words model.\n\nYou are encouraged to explore alternative hand-crafted features, and are welcome to use any third-party library to implement them, as long as they do not use a separate model for their computation (no large language models or word2vec!).\n\nPlease ensure you have completed the following before your final submission.\n\nHave you copied and hand-signed the honor code specified in Question 1?\n\nHave you listed all students (Names and ID numbers) that you collaborated with?\n\nHave you included your Kaggle Score and Kaggle Username for both questions 8.4 and 8.5?\n\nHave you provided a code appendix including all code you wrote in solving the homework?\n\nHave you included featurize.py in your code appendix if you modified it?\n\nHave you created an archive containing all \u201c.py\u201d files that you wrote or modified to generate your homework solutions (including featurize.py if you modified it)?\n\nHave you removed all data and extraneous files from the archive?\n\nHave you included a README file in your archive briefly describing how to run your code on the test data and reproduce your results?\n\nHave you submitted your written solutions to the Gradescope assignment titled HW3 Write- Up and selected pages appropriately?\n\nHave you submitted your executable code archive to the Gradescope assignment titled HW3 Code ?\n\nHave you submitted your test set predictions for both MNIST and SPAM to the appropriate Kaggle challenges?\n\nCongratulations!\n\nYou have completed Homework 3.\n\nYou are given sample of n observations, each with d features [aka predictors].\n\nSome observations belong to class C; some do not.\n\nPredict whether future borrowers will default, based on their income & age.\n\nRepresent each observation as a point in d -dimensional space, called a sample point / a feature vector / independent variables.\n\nThen we use these curves to predict which future borrowers will default.\n\nWhen sinuous decision boundary \ufb01ts sample points so well that it doesn\u2019t classify future points well.\n\nAka predictor function.\n\nUsually uses a linear decision function.\n\nThink of x as a point in 5-dimensional space.\n\nH = { x : w \u00b7 x = \ufffd \u21b5 } .\n\nThe set H is called a hyperplane.\n\nLet x , y be 2 points that lie on H .\n\nThen w \u00b7 ( y \ufffd x ) = 0.\n\nIf w is a unit vector, then w \u00b7 x + \u21b5 is the signed distance from x to H . I.e., positive on w \u2019s side of H ; negative on other side.\n\nMoreover, the distance from H to the origin is \u21b5 .\n\nHence \u21b5 = 0 if and only if H passes through origin.\n\nIf w is not a unit vector, w \u00b7 x + \u21b5 is the signed distance times some real.\n\nThe coe \ufffd cients in w , plus \u21b5 , are called weights (or parameters or regression coe \ufffd cients).\n\nThe input data is linearly separable if there exists a hyperplane that separates all the sample points in class C from all the points NOT in class C.\n\nObviously, if your data are not linearly separable, a linear classi\ufb01er cannot do a perfect job.\n\nNote that this is hardly the worst example I could have given.\n\nSlow, but correct for linearly separable points.\n\nUses a numerical optimization algorithm, namely, gradient descent.\n\nHow many of you know what gradient descent is?\n\nHow many of you know what a linear program is?\n\nHow many of you know what the simplex algorithm for linear programming is?\n\nHow many of you know what a quadratic program is?\n\nWe\u2019re going to learn what most of these things are.\n\nAs machine learning people, we will be heavy users of optimization methods.\n\nUnfortunately, I won\u2019t have time to teach you algorithms for many optimization problems, but we\u2019ll learn a few.\n\nFor simplicity, consider only decision boundaries that pass through the origin.\n\nWe de\ufb01ne a risk function R that is positive if some constraints are violated.\n\nThen we use optimization to choose w that minimizes R .\n\nBut if z has the wrong sign, the loss function is positive.\n\nOtherwise, R ( w ) is positive, and we want to \ufb01nd a better w .\n\nFind w that minimizes R ( w ).\n\nEvery point in the dark green \ufb02at spot is a minimum.\n\nBut we\u2019ve transformed this into a problem of \ufb01nding an optimal point in a di \u21b5 erent space, which I\u2019ll call w -space.\n\nPoint x lies on hyperplane { z : w \u00b7 z = 0 } , w \u00b7 x = 0 , point w lies on hyperplane { z : x \u00b7 z = 0 } in w -space.\n\nFor a point x not in class C (marked by an X), w and x must be on opposite sides of the hyperplane that x transforms into.\n\nWe have switched from the problem of \ufb01nding a hyperplane in x - space to the problem of \ufb01nding a point in w -space.\n\nAn optimization algorithm: gradient descent on R .\n\nGiven a starting point w , \ufb01nd gradient of R with respect to w ; this is the direction of steepest ascent.\n\nTake a step in the opposite direction.\n\nAt any point w , we walk downhill in direction of steepest descent, \ufffdr R ( w ).\n\nEach step takes O ( nd ) time.\n\nCalled the perceptron algorithm.\n\nEach step takes O ( d ) time.\n\nHowever, stochastic gradient descent does not work for every problem that gradient descent works for.\n\nWhat if separating hyperplane doesn\u2019t pass through origin?\n\nAdd a \ufb01ctitious dimension.\n\nRun perceptron algorithm in ( d + 1)-dimensional space.\n\nIt was originally designed not to be a program, but to be implemented in hardware for image recognition on a 20 \u21e5 20 pixel image.\n\nMark I perceptron.jpg (from Wikipedia, \u201cPerceptron\u201d) [The Mark I Perceptron Machine.\n\nThe algorithm gets slower if \u270f is too small because it has to take lots of steps to get down the hill.\n\nThere\u2019s no reliable way to choose a good step size \u270f .\n\nFortunately, optimization algorithms have improved a lot since 1957.\n\nYou can get rid of the step size by using a decent modern \u201cline search\u201d al- gorithm.\n\nThe margin of a linear classi\ufb01er is the distance from the decision boundary to the nearest sample point.\n\nWhat if we make the margin as wide as possible?\n\nTo maximize the margin, minimize k w k .\n\nCalled a quadratic program in d + 1 dimensions and n constraints.\n\nIt has one unique solution!\n\nThe solution gives us a maximum margin classi\ufb01er, aka a hard-margin support vector machine (SVM).\n\nThe SVM is looking for the point nearest the origin that lies above the blue plane (representing an in-class training point) but below the red and pink planes (representing out-of-class training points).\n\nIn this example, that optimal point lies where the three planes intersect.\n\nAllow some points to violate the margin, with slack variables.\n\nRe-de\ufb01ne \u201cmargin\u201d to be 1 / k w k .\n\nTo prevent abuse of slack, we add a loss term to", "doc_id": "176ff7c6-17f6-47ef-9267-35ba18621133", "embedding": null, "extra_info": null, "index": 2, "child_indices": [], "ref_doc_id": "bfac5f31-d3df-41d7-87f7-aa808e930d69", "node_info": {"start": 13634, "end": 20803}}, "5d8fccea-cbf6-4c4f-9ca6-d87897c9e5ef": {"text": "as possible?\n\nTo maximize the margin, minimize k w k .\n\nCalled a quadratic program in d + 1 dimensions and n constraints.\n\nIt has one unique solution!\n\nThe solution gives us a maximum margin classi\ufb01er, aka a hard-margin support vector machine (SVM).\n\nThe SVM is looking for the point nearest the origin that lies above the blue plane (representing an in-class training point) but below the red and pink planes (representing out-of-class training points).\n\nIn this example, that optimal point lies where the three planes intersect.\n\nAllow some points to violate the margin, with slack variables.\n\nRe-de\ufb01ne \u201cmargin\u201d to be 1 / k w k .\n\nTo prevent abuse of slack, we add a loss term to objective fn.\n\nUse validation to choose C .\n\nThe further a point penetrates the margin, the bigger the \ufb01ne you have to pay.\n\nWe want to make the margin as wide as possible, but we also want to spend as little money as possible.\n\nIf the regularization parameter C is small, it means we\u2019re willing to spend lots of money on violations so we can get a wider margin.\n\nIf C is big, it means we\u2019re cheap and we won\u2019t pay much for violations, even though we\u2019ll su \u21b5 er a narrower margin.\n\nHow to do nonlinear decision boundaries?\n\nMake nonlinear features that lift points into a higher-dimensional space.\n\nHigh- d linear classi\ufb01er !\n\nFind a linear classi\ufb01er in \ufffd -space.\n\nIt induces a sphere classi\ufb01er in x -space.\n\nHence points inside sphere $ same side of hyperplane in \ufffd -space.\n\nA hyperplane is essentially a hypersphere with in\ufb01nite radius.\n\nSo hypersphere decision boundaries can do everything hyperplane decision boundaries can do, plus a lot more.\n\nIsosurface de\ufb01ned by this equation is called a quadric.\n\nIn the special case of two dimensions, it\u2019s also known as a conic section.\n\nIf the dimension is large, these feature vectors are getting huge, and that\u2019s going to impose a serious computational cost.\n\nArtist\u2019s conception; these aren\u2019t actual calculations, just hand-drawn guesses.\n\nIt\u2019s a balancing act between under\ufb01tting and over\ufb01tting.\n\nBut features can get much more complicated than polynomials, and they can be tailored to \ufb01t a speci\ufb01c problem.\n\nCollect line orientations in local histograms (each having 12 orientation bins per region); use histograms as features ( instead of raw pixels).\n\nMaji & Malik, 2009.\n\nUse Ed Discussion for public and private questions that can be viewed by all the TAs.\n\nI check Ed Discussion far more often and reliably than email.\n\nFor very personal issues, send email to jrs@berkeley.edu.\n\nTBA and by appointment.\n\nThis class introduces algorithms for learning, which constitute an important part of artificial intelligence.\n\nBoth textbooks for this class are available free online.\n\nHardcover and eTextbook versions are also available.\n\nYou have a total of 5 slip days that you can apply to your semester's homework.\n\nWe will simply not award points for any late homework you submit that would bring your total slip days over five.\n\nIf you are in the Disabled Students' Program and you are offered an extension, even with your extension plus slip days combined, no single assignment can be extended more than 5 days.\n\nThe following homework due dates and midterm date are tentative and may change.\n\nThe CS 289A Project has a proposal due Friday, April 7.\n\nThe video is due Monday, May 8, and the final report is due Tuesday, May 9.\n\nPlease sign up your group for a ten-minute meeting slot with one of the TAs.\n\nDates are available from <b>April 11 to April 15</b>.\n\nIf you need serious computational resources, our former Teaching Assistant Alex Le-Tu has written lovely guides to using Google Cloud and using Google Colab.\n\nHomework 1 is due Wednesday, January 25 at 11:59 PM.\n\nWarning: 200 MB zipfile.\n\nHomework 2 is due Wednesday, February 8 at 11:59 PM.\n\nHomework 3 is due Wednesday, February 22 at 11:59 PM.\n\nWarning: 22 MB zipfile.\n\nHomework 4 is due Wednesday, March 8 at 11:59 PM.\n\nHomework 5 is due Wednesday, March 22 at 11:59 PM.\n\nHomework 6 is due Wednesday, April 19 at 11:59 PM.\n\nHomework 7 is due Wednesday, May 3 at 11:59 PM.\n\nThe Midterm will take place on Monday, March 20 at 7:00\u20138:30 PM in Wheeler Auditorium (150 Wheeler Hall).\n\nPrevious midterms are available: Without solutions: Spring 2013, Spring 2014, Spring 2015, Fall 2015, Spring 2016, Spring 2017, Spring 2019, Summer 2019, Spring 2020 Midterm A, Spring 2020 Midterm B, Spring 2021, Spring 2022.\n\nWith solutions: Spring 2013, Spring 2014, Spring 2015, Fall 2015, Spring 2016, Spring 2017, Spring 2019, Summer 2019, Spring 2020 Midterm A, Spring 2020 Midterm B, Spring 2021, Spring 2022.\n\nThe Final Exam will take place on Friday, May 12 at 3\u20136 PM.\n\nPrevious final exams are available.\n\nWithout solutions: Spring 2013, Spring 2014, Spring 2015, Fall 2015, Spring 2016, Spring 2017, Spring 2019, Spring 2020, Spring 2021, Spring 2022.\n\nWith solutions: Spring 2013, Spring 2014, Spring 2015, Fall 2015, Spring 2016, Spring 2017, Spring 2019, Spring 2020, Spring 2021, Spring 2022.\n\nThe references below to sections in Introduction to Statistical Learning with Applications in R (ISL) are for the first edition.\n\nI will update them to the second edition when time permits.\n\nLecture 1 (January 18): Introduction.\n\nClassification, training, and testing.\n\nValidation and overfitting.\n\nRead ESL, Chapter 1.\n\nMy lecture notes (PDF).\n\nThe lecture video.\n\nIn case you don't have access to bCourses, here's the captioned version of the screencast (screen only).\n\nLecture 2 (January 23): Linear classifiers.\n\nDecision functions and decision boundaries.\n\nThe centroid method. Perceptrons.\n\nRead parts of the Wikipedia Perceptron page.\n\nOptional: Read ESL, Section 4.5\u20134.5.1.\n\nMy lecture notes (PDF).\n\nThe lecture video.\n\nIn case you don't have access to bCourses, here's the captioned version of the screencast (screen only).\n\nLecture 3 (January 25): Gradient descent, stochastic gradient descent, and the perceptron learning algorithm.\n\nFeature space versus weight space.\n\nThe maximum margin classifier, aka hard-margin support vector machine (SVM).\n\nRead ISL, Section 9\u20139.1.\n\nMy lecture notes (PDF).\n\nThe lecture video.\n\nIn case you don't have access to bCourses, here's the captioned version of the screencast (screen only).\n\nLecture 4 (January 30): The support vector classifier, aka soft-margin support vector machine (SVM).\n\nFeatures and nonlinear decision boundaries.\n\nRead ESL, Section 12.2 up to and including the first paragraph of 12.2.1.\n\nMy lecture notes (PDF).\n\nThe lecture video.\n\nIn case you don't have access to bCourses, here's the captioned version of the screencast (screen only).\n\nLecture 5 (February 1): Machine learning abstractions: application/data, model, optimization problem, optimization algorithm.\n\nCommon types of optimization problems: unconstrained, constrained (with equality constraints), linear programs, quadratic programs, convex programs. Optional: Read (selectively) the Wikipedia page on mathematical optimization.\n\nMy lecture notes (PDF).\n\nThe lecture video.\n\nIn case you don't have access to bCourses, here's the captioned version of the screencast (screen only).\n\nLecture 6 (February 6): Decision theory: the Bayes", "doc_id": "5d8fccea-cbf6-4c4f-9ca6-d87897c9e5ef", "embedding": null, "extra_info": null, "index": 3, "child_indices": [], "ref_doc_id": "bfac5f31-d3df-41d7-87f7-aa808e930d69", "node_info": {"start": 20852, "end": 28004}}, "b5619965-5220-4f6d-8065-59880da12e08": {"text": "lecture notes (PDF).\n\nThe lecture video.\n\nIn case you don't have access to bCourses, here's the captioned version of the screencast (screen only).\n\nLecture 5 (February 1): Machine learning abstractions: application/data, model, optimization problem, optimization algorithm.\n\nCommon types of optimization problems: unconstrained, constrained (with equality constraints), linear programs, quadratic programs, convex programs. Optional: Read (selectively) the Wikipedia page on mathematical optimization.\n\nMy lecture notes (PDF).\n\nThe lecture video.\n\nIn case you don't have access to bCourses, here's the captioned version of the screencast (screen only).\n\nLecture 6 (February 6): Decision theory: the Bayes decision rule and optimal risk.\n\nGenerative and discriminative models.\n\nRead ISL, Section 4.4.1.\n\nMy lecture notes (PDF).\n\nThe lecture video.\n\nIn case you don't have access to bCourses, here's a backup screencast (screen only).\n\nLecture 7 (February 8): Gaussian discriminant analysis, including quadratic discriminant analysis (QDA) and linear discriminant analysis (LDA).\n\nMaximum likelihood estimation (MLE) of the parameters of a statistical model.\n\nFitting an isotropic Gaussian distribution to sample points.\n\nRead ISL, Section 4.4.\n\nOptional: Read (selectively) the Wikipedia page on maximum likelihood estimation.\n\nMy lecture notes (PDF).\n\nThe lecture video.\n\nIn case you don't have access to bCourses, here's a backup screencast (screen only).\n\nLecture 8 (February 13): Eigenvectors, eigenvalues, and the eigendecomposition.\n\nThe Spectral Theorem for symmetric real matrices.\n\nThe quadratic form and ellipsoidal isosurfaces as an intuitive way of understanding symmetric matrices.\n\nApplication to anisotropic normal distributions (aka Gaussians).\n\nRead Chuong Do's notes on the multivariate Gaussian distribution.\n\nMy lecture notes (PDF).\n\nThe lecture video.\n\nHowever, the screen recording failed, so you should probably watch my 2021 lecture instead.\n\nLecture 9 (February 15): Anisotropic normal distributions (aka Gaussians).\n\nMLE, QDA, and LDA revisited for anisotropic Gaussians.\n\nRead ISL, Sections 4.4 and 4.5.\n\nMy lecture notes (PDF).\n\nThe lecture video.\n\nIn case you don't have access to bCourses, here's a backup screencast (screen only).\n\nFebruary 20 is Presidents' Day.\n\nLecture 10 (February 22): Regression: fitting curves to data.\n\nThe 3-choice menu of regression function + loss function + cost function.\n\nLeast-squares linear regression as quadratic minimization.\n\nThe design matrix, the normal equations, the pseudoinverse, and the hat matrix (projection matrix).\n\nLogistic regression; how to compute it with gradient descent or stochastic gradient descent.\n\nRead ISL, Sections 4\u20134.3.\n\nLecture 11 (February 27): Newton's method and its application to logistic regression. LDA vs.\n\nWeighted least-squares regression.\n\nLeast-squares polynomial regression.\n\nRead ISL, Sections 4.4.3, 7.1, 9.3.3; ESL, Section 4.4.1.\n\nOptional: here is a fine short discussion of ROC curves\u2014but skip the incoherent question at the top and jump straight to the answer.\n\nLecture 12 (March 1): Statistical justifications for regression.\n\nThe empirical distribution and empirical risk.\n\nHow the principle of maximum likelihood motivates the cost functions for least-squares linear regression and logistic regression.\n\nThe bias-variance decomposition; its relationship to underfitting and overfitting; its application to least-squares linear regression.\n\nRead ESL, Sections 2.6 and 2.9.\n\nOptional: Read the Wikipedia page on the bias-variance trade-off.\n\nLecture 13 (March 6): Ridge regression: penalized least-squares regression for reduced overfitting.\n\nHow the principle of maximum a posteriori (MAP) motivates the penalty term (aka Tikhonov regularization). Subset selection.\n\nLasso: penalized least-squares regression for reduced overfitting and subset selection.\n\nRead ISL, Sections 6\u20136.1.2, the last part of 6.1.3 on validation, and 6.2\u20136.2.1; and ESL, Sections 3.4\u20133.4.3.\n\nOptional: This CrossValidated page on ridge regression is pretty interesting.\n\nLecture 14 (March 8): Decision trees; algorithms for building them.\n\nEntropy and information gain.\n\nRead ISL, Sections 8\u20138.1.\n\nLecture 15 (March 13): More decision trees: multivariate splits; decision tree regression; stopping early; pruning.\n\nEnsemble learning: bagging (bootstrap aggregating), random forests.\n\nRead ISL, Section 8.2.\n\nLecture 16 (March 15): Kernels.\n\nKernel ridge regression.\n\nThe polynomial kernel. Kernel perceptrons.\n\nKernel logistic regression.\n\nThe Gaussian kernel.\n\nOptional: Read ISL, Section 9.3.2 and ESL, Sections 12.3\u201312.3.1 if you're curious about kernel SVM.\n\nThe Midterm will take place on Monday, March 20 at 7:00\u20138:30 PM in Wheeler Auditorium (150 Wheeler Hall).\n\nThe midterm covers Lectures 1\u201313, the associated readings listed on the class web page, Homeworks 1\u20134, and discussion sections related to those topics.\n\nLecture 17 (March 22): Neural networks.\n\nGradient descent and the backpropagation algorithm.\n\nRead ESL, Sections 11.3\u201311.4.\n\nOptional: Welch Labs' video tutorial Neural Networks Demystified on YouTube is quite good (note that they transpose some of the matrices from our representation).\n\nAlso of special interest is this Javascript neural net demo that runs in your browser.\n\nHere's another derivation of backpropagation that some people have found helpful.\n\nMarch 27\u201331 is Spring Recess.\n\nLecture 18 (April 3): Neuron biology: axons, dendrites, synapses, action potentials.\n\nDifferences between traditional computational models and neuronal computational models.\n\nBackpropagation with softmax outputs and logistic loss.\n\nUnit saturation, aka the vanishing gradient problem, and ways to mitigate it.\n\nOptional: Try out some of the Javascript demos on this excellent web page\u2014and if time permits, read the text too.\n\nThe first four demos illustrate the neuron saturation problem and its fix with the logistic loss (cross-entropy) functions.\n\nThe fifth demo gives you sliders so you can understand how softmax works.\n\nLecture 19 (April 5): Heuristics for faster training.\n\nHeuristics for avoiding bad local minima.\n\nHeuristics to avoid overfitting.\n\nConvolutional neural networks.\n\nNeurology of retinal ganglion cells in the eye and simple and complex cells in the V1 visual cortex.\n\nRead ESL, Sections 11.5 and 11.7.\n\nHere is the video about Hubel and Wiesel's experiments on the feline V1 visual cortex.\n\nHere is Yann LeCun's video demonstrating LeNet5.\n\nOptional: A fine paper on heuristics for better neural network learning is Yann LeCun, Leon Bottou, Genevieve B.\n\nOrr, and Klaus-Robert M\u00fcller, \u201cEfficient BackProp,\u201d in G.\n\nOrr and K.-R. M\u00fcller (Eds.), Neural Networks: Tricks of the Trade, Springer, 1998.\n\nAlso of special interest is this Javascript convolutional", "doc_id": "b5619965-5220-4f6d-8065-59880da12e08", "embedding": null, "extra_info": null, "index": 4, "child_indices": [], "ref_doc_id": "bfac5f31-d3df-41d7-87f7-aa808e930d69", "node_info": {"start": 27958, "end": 34743}}, "9a161fa2-669f-4b90-8b28-76c8cac0586b": {"text": "neural networks.\n\nNeurology of retinal ganglion cells in the eye and simple and complex cells in the V1 visual cortex.\n\nRead ESL, Sections 11.5 and 11.7.\n\nHere is the video about Hubel and Wiesel's experiments on the feline V1 visual cortex.\n\nHere is Yann LeCun's video demonstrating LeNet5.\n\nOptional: A fine paper on heuristics for better neural network learning is Yann LeCun, Leon Bottou, Genevieve B.\n\nOrr, and Klaus-Robert M\u00fcller, \u201cEfficient BackProp,\u201d in G.\n\nOrr and K.-R. M\u00fcller (Eds.), Neural Networks: Tricks of the Trade, Springer, 1998.\n\nAlso of special interest is this Javascript convolutional neural net demo that runs in your browser.\n\nSome slides about the V1 visual cortex and ConvNets (PDF).\n\nLecture 20 (April 10): Unsupervised learning.\n\nPrincipal components analysis (PCA).\n\nDerivations from maximum likelihood estimation, maximizing the variance, and minimizing the sum of squared projection errors.\n\nEigenfaces for face recognition.\n\nRead ISL, Sections 10\u201310.2 and the Wikipedia page on Eigenface.\n\nOptional: Watch the video for Volker Blanz and Thomas Vetter's A Morphable Model for the Synthesis of 3D Faces.\n\nLecture 21 (April 12): The singular value decomposition (SVD) and its application to PCA.\n\nClustering: k-means clustering aka Lloyd's algorithm; k-medoids clustering; hierarchical clustering; greedy agglomerative clustering. Dendrograms. Read ISL, Section 10.3.\n\nLecture 22 (April 17): The geometry of high-dimensional spaces. Random projection.\n\nThe pseudoinverse and its relationship to the singular value decomposition.\n\nOptional: Mark Khoury, Counterintuitive Properties of High Dimensional Space.\n\nOptional: The Wikipedia page on the Moore\u2013Penrose inverse.\n\nFor reference: Sanjoy Dasgupta and Anupam Gupta, An Elementary Proof of a Theorem of Johnson and Lindenstrauss, Random Structures and Algorithms 22(1)60\u201365, January 2003.\n\nLecture 23 (April 19): Learning theory.\n\nRange spaces (aka set systems) and dichotomies.\n\nThe shatter function and the Vapnik\u2013Chervonenkis dimension.\n\nRead Andrew Ng's CS 229 lecture notes on learning theory.\n\nFor reference: Thomas M.\n\nCover, Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition, IEEE Transactions on Electronic Computers 14(3):326\u2013334, June 1965.\n\nLecture 24 (April 24): AdaBoost, a boosting method for ensemble learning.\n\nNearest neighbor classification and its relationship to the Bayes risk.\n\nRead ESL, Sections 10\u201310.5, and ISL, Section 2.2.3.\n\nFor reference: Yoav Freund and Robert E.\n\nSchapire, A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, Journal of Computer and System Sciences 55(1):119\u2013139, August 1997.\n\nFreund and Schapire's G\u00f6del Prize citation and their ACM Paris Kanellakis Theory and Practice Award citation.\n\nFor reference: Thomas M.\n\nCover and Peter E.\n\nHart, Nearest Neighbor Pattern Classification, IEEE Transactions on Information Theory 13(1):21\u201327, January 1967.\n\nFor reference: Evelyn Fix and J. L. Hodges Jr., Discriminatory Analysis---Nonparametric Discrimination: Consistency Properties, Report Number 4, Project Number 21-49-004, US Air Force School of Aviation Medicine, Randolph Field, Texas, 1951.\n\nSee also This commentary on the Fix\u2013Hodges paper.\n\nLecture 25 (April 26): The exhaustive algorithm for k-nearest neighbor queries.\n\nSpeeding up nearest neighbor queries.\n\nVoronoi diagrams and point location. k-d trees.\n\nApplication of nearest neighbor search to the problem of geolocalization: given a query photograph, determine where in the world it was taken.\n\nIf I like machine learning, what other classes should I take?\n\nFor reference: the best paper I know about how to implement a k-d tree is Sunil Arya and David M.\n\nMount, Algorithms for Fast Vector Quantization, Data Compression Conference, pages 381\u2013390, March 1993.\n\nFor reference: the IM2GPS web page, which includes a link to the paper.\n\nThe Final Exam will take place on Friday, May 12, 3\u20136 PM.\n\nSections begin to meet on January 24.\n\nSome of our office hours are online or hybrid.\n\nTo attend an online office hour, submit a ticket to the Online Office Hour Queue at https://oh.eecs189.org and attend the office hour through Zoom at this Zoom link.\n\nPlease read the <a href=\"https://piazza.com/class/kxznj3t030x34a?cid=49\">online office hour rules in this Piazza post</a>.\n\nSupported in part by the National Science Foundation under Awards CCF-0430065, CCF-0635381, IIS-0915462, CCF-1423560, and CCF-1909204, in part by a gift from the Okawa Foundation, and in part by an Alfred P.\n\nSloan Research Fellowship.\n\nSome of our office hours are online or hybrid.\n\nTo attend an online office hour, submit a ticket to the Online Office Hour Queue at https://oh.eecs189.org and attend the office hour through Zoom at this Zoom link.\n\nCS 189/289A Introduction to Machine Learning Jonathan Shewchuk Spring 2023 Mondays and Wednesdays, 6:30\u20138:00 pm Hearst Field Annex A1 Begins Wednesday, January 18 Discussion sections begin Tuesday, January 24 Contact: Use Ed Discussion for public and private questions that can be viewed by all the TAs.\n\nI check Ed Discussion far more often and reliably than email.\n\nFor very personal issues, send email to jrs@berkeley.edu.\n\nMy office hours: TBA and by appointment.\n\nCS 189/289A Introduction to Machine Learning Jonathan Shewchuk Spring 2023 Mondays and Wednesdays, 6:30\u20138:00 pm Hearst Field Annex A1 Begins Wednesday, January 18 Discussion sections begin Tuesday, January 24 Contact: Use Ed Discussion for public and private questions that can be viewed by all the TAs.\n\nI check Ed Discussion far more often and reliably than email.\n\nFor very personal issues, send email to jrs@berkeley.edu.\n\nMy office hours: TBA and by appointment.\n\n", "doc_id": "9a161fa2-669f-4b90-8b28-76c8cac0586b", "embedding": null, "extra_info": null, "index": 5, "child_indices": [], "ref_doc_id": "bfac5f31-d3df-41d7-87f7-aa808e930d69", "node_info": {"start": 34756, "end": 40510}}}, "rel_map": {"you": [["classifier", "want"]], "classifier": [["height", "takes"], ["classification", "returns"], ["Gaussian discriminant analysis", "based on"]], "training set": [["5'5\" women", "includes"], ["5'5\" men", "includes"]], "feature space": [["two training points", "has"]], "probabilistic classifier": [["desirable", "is"]], "false negative": [["somebody misses an early diagnosis", "might mean"]], "false positive": [["somebody gets an unnecessary treatment", "might mean"]], "loss function": [["badness", "specifies"]], "Bayes risk": [["risk of the Bayes classifier", "is"]], "TEX": [["computer software", "is"]], "Kaggle": [["online platform", "is"]], "submitting to Kaggle": [["you to get feedback", "allows"]], "consequences of academic misconduct": [["particularly severe", "are"]], "file": [["Kaggle", "submitted to"]], "decision boundary": [["x=b", "is"]], "value": [["part 1", "compares to"]], "isocontours": [["functions", "plots"]], "domain": [["characteristics", "shows"]], "eigenvectors": [["eigenvalues", "have magnitudes"]], "covariance matrix": [["singular", "is"]], "mean": [["PDF", "maximizes"]], "Kaggle competition": [["gradient descent", "uses"], ["linear decision function", "uses"], ["Bag-Of-Words model", "uses"], ["optimization methods", "uses"], ["stochastic gradient descent", "uses"], ["line search algorithm", "uses"], ["quadratic program", "uses"], ["support vector machine", "uses"]], "decision theory": [["Bayes", "is"]], "Lecture 5": [["Machine learning abstractions", "is about"], ["unconstrained", "common types of optimization problems"], ["constrained", "common types of optimization problems"], ["linear programs", "common types of optimization problems"], ["quadratic programs", "common types of optimization problems"], ["convex programs", "common types of optimization problems"]], "Lecture 6": [["the Bayes decision rule", "Decision theory"], ["optimal risk", "Decision theory"]], "CS 189/289A Introduction to Machine Learning": [["class", "is"], ["Jonathan Shewchuk", "taught by"], ["Mondays and Wednesdays", "meets"], ["January 18", "begins"], ["January 24", "has discussion sections that begin"], ["that can be viewed by all the TAs", "uses Ed Discussion for questions"], ["TBA and by appointment", "has office hours"]]}, "__type__": "kg"}}}}